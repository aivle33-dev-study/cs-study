# Ch12. 로드 밸런서

---

**Contents**

12.1 부하 분산이란?

12.2 부하 분산 방법

12.3 헬스 체크

12.4 부하 분산 알고리즘

12.5 로드 밸런서 구성 방식

12.6 로드 밸런서 동작 모드

12.7 로드 밸런서 유의사항

12.8 HAProxy를 사용한 로드 밸런서 설정

---

서비스의 안정성이나 가용량을 높이기 위해 서비스를 이중화할 때는 서비스 자체적으로 HA 클러스터(High Availability Cluster)를 구성하기도 하지만 복잡한 고려 없이 이중화를 손쉽게 구현하도록 **로드 밸런서**가 많이 사용됨

**로드 밸런서**는 다양한 구성 방식과 동작 모드가 있으며 각 방식과 모드에 따라 서비스 흐름이나 패킷 내용이 달라짐

# 12.1 부하 분산이란?

서비스 규모가 커지면 물리나 가상 서버 한 대로는 모든 서비스를 수용할 수 없음

보통 하나의 서비스는 가용성을 높이기 위해 두 대 이상의 서버로 구성하는데 서버 IP 주소가 다르므로 어떤 IP로 서비스를 요청할지 결정해야 함

사용자에 따라 호출하는 서버의 IP가 다르면 특정 서버에 장애가 발생했을 때, 전체 사용자에게는 영향을 미치지는 않지만 여전히 부분적으로 서비스 장애가 발생

이런 문제점을 해결하기 위해 L4나 L7 스위치라는 로드 밸런서(Load Balancer)를 사용

로드 밸런서에는 동일한 서비스를 하는 다수의 서버가 등록되고 사용자로부터 서비스 요청이 오면 로드 밸런서가 받아 사용자별로 **다수의 서버에 서비스 요청을 분산시켜 부하를 분산**

로드 밸런서에서는 서비스를 위한 가상 IP(VIP)를 하나 제공하고 사용자는 각 서버의 개별 IP 주소가 아닌 **동일한 가상 IP를 통해 각 서버로 접근**

이 외에도 로드 밸런서는 **각 서버의 서비스 상태를 체크해 서비스가 가능한 서버로만 사용자의 요청을 분산**하므로 서버에서 장애가 발생하더라도 기존 요청을 분산하여 다른 서버에서 서비스를 제공할 수 있음

c.f. *FWLB*

서버에 대한 부하 분산뿐만 아니라 방화벽을 active-active로 구성하기 위해 로드 밸런서를 사용하기도 함

**서버 부하 분산을 SLB(Server Load Balancing), 방화벽 부하 분산을 FWLB(FireWall Load Balancing)**라고 함

방화벽은 자신을 통과한 패킷에 대해 **세션을 관리하는 테이블**을 가짐

응답 패킷은 방화벽 정책을 바로 확인하는 것이 아닌 세션 테이블에서 해당 패킷을 먼저 조회

세션 테이블에 있는 응답 패킷이라면 이미 정책에서 허용된 패킷이므로 방화벽을 바로 통과

하지만 세션 테이블에 응답 패킷이 없다면 요청한 적이 없는 패킷에 대한 응답으로 간주하고 공격성으로 판단해 **해당 패킷은 폐기(Drop)**

**이런 경우는 출발지와 목적지 간 경로가 두 개 이상 있어 비대칭 경로가 만들어질 때도 발생할 수 있음**

방화벽 장비를 이중화할 경우, 이런 비대칭 동작으로 인해 방화벽이 정상적으로 동작하지 않을 수 있음

이런 문제를 해결하고 동시에 이중화된 방화벽을 모두 사용하기 위해 FWLB가 사용됨****

**FWLB가 세션을 인식하고 일정한 규칙을 이용하여 방화벽 세션을 분산하는데 (해시알고리즘 이용) 한 번 방화벽을 지나갔던 세션이 다시 같은 방화벽을 거치도록 트래픽을 분산**

FWLB를 이용하더라도 방화벽에 장애가 발생하는 경우를 대비하기 위해 방화벽에서 설정이 필요

방화벽끼리 세션 테이블을 동기화하거나 방화벽에서 첫 번째 패킷이 SYN이 아니어도 허용하는 기능을 사용해 방화벽의 장애로 인해 기존 세션 테이블에 없던 트래픽이 들어오더라도 처리할 수 있도록 설정

# 12.2 부하 분산 방법

LACP는 다수의 물리 인터페이스를 하나의 논리 인터페이스로 구성하기 위해 **LACP를 위한 가상의 MAC 주소를 만듦**

로드 밸런서도 이와 유사하게 **부하를 다수의 장비에 분산시키기 위해 가상 IP 주소를 가짐**

이 IP 주소는 가상 IP 주소이므로 VIP(Virtual IP)라고도 하고 서비스를 위해 사용되는 IP 주소이므로 서비스 IP 주소라고도 함

각 서버의 실제 IP 주소를 리얼(Real) IP라고 하고 **로드 밸런서의 가상 IP에 실제 서버들이 바인딩(Binding)**됨

<img width="784" alt="NW_pic_12-4" src="https://github.com/aivle33-dev-study/cs-study/assets/90406411/e2f431b4-7425-4598-bf98-e0d88385d6f0">

**부하 분산 예시**

서버 3대 중,

**1번 서버 10.10.20.11 http 서비스 데몬**

**2번 서버 10.10.20.12 http, https 서비스 데몬 모두 동작**

**3번 서버 10.10.20.13 https 서비스 데몬**

사용자가 http와 https서비스로 접근하기 위한 **VIP 주소인 10.10.10.1이 로드 밸런서에 설정**

VIP에는 사용자의 서비스 요청이 들어올 때, 어느 서버로 요청을 전달할 것인지 **부하 분산 그룹을 설정**

여기서 http 서비스는 서버 1번과 2번으로 https 서비스는 서버 2번과 3번으로 부하 분산 그룹 설정

로드밸런서에서 부하 분산을 위한 그룹을 만들 때는 **OSI 3계층 정보인 IP 주소 뿐만 아니라 4계층 정보인 서비스 포트까지 지정**해 만듦

그래서 **로드 밸런서를 L4 스위치**라고도 함

7계층 정보까지 확인해 처리하는 기능이 포함되는 경우도 있어 **L7 스위치**라고도 하지만 **보통 로드 밸런서를 L4 스위치**라고 부름

예제에서 HTTP와 HTTPS 서비스에 대해 각각 동일한 VIP를 사용했지만 **서로 다른 VIP**로도 구성할 수 있음

또한, 로드 밸런서의 VIP에 설정된 서비스 포트와 실제 서버의 서비스 포트는 반드시 같을 필요가 없음

즉, 실제 서버에서는 서비스 포트 8080으로 웹 서비스를 수행하면서 VIP에서는 일반 HTTP 서비스 포트인 80으로 설정 가능

# 12.3 헬스 체크

로드 밸런서에서는 부하 분산을 하는 각 서버의 서비스를 주기적으로 **헬스 체크(Health Check)**해 정상적인 서비스 쪽으로만 부하를 분산하고 비정상적인 서버는 서비스 그룹에서 제외해 트래픽을 보내지 않음

이 후에 헬스 체크를 계속 수행해 다시 정상으로 확인되면 서비스 그룹에 해당 장비를 다시 넣어 트래픽이 서버 쪽으로 보내지도록 해줌

## 12.3.1 헬스 체크 방식

1. **ICMP**
- **VIP에 연결된 리얼 서버에 대해 ICMP(ping)로 헬스 체크를 수행하는 방법**
- **단순히 서버가 살아 있는지 여부만 체크**하는 방법이므로 잘 사용하지 않음

2. **TCP 서비스 포트**
- 가장 기본적인 헬스 체크 방법은 **로드 밸런서에 설정된 서버의 서비스 포트를 확인하는 것**
- 즉, 로드 밸런서에서 서버의 서비스 포트 2000번을 등록했다면 로드 밸런서에서는 리얼 IP의 2000번 포트로 SYN을 보내고 해당 리얼 IP를 가진 서버로부터 SYN, ACK을 받으면 서버에 다시 ACK로 응답하고 FIN을 보내 헬스 체크를 종료

3. **TCP 서비스 포트 : Half Open**
- 일반 TCP 서비스 포트를 확인할 때는 SYN/SYN,ACK/ACK까지 정상적인 **3방향 핸드셰이크**를 거침
- 헬스 체크로 인한 부하를 줄이거나 정상적인 종료 방식보다 빨리 헬스 체크 세션을 끊기 위해 정상적인 핸드셰이크가 아닌 **TCP Half Open(절반 개방) 방식**을 사용하기도 함
- **TCP Half Open 방식은 SYN을 보내고 SYN, ACK을 받지만 ACK 대신 RST를 보내 세션을 끊음**

4. **HTTP 상태 코드**
- 웹 서비스를 할때, 서비스 포트까지는 TCP로 정상적으로 열리지만 **웹 서비스에 대한 응답을 정상적으로 해주지 못하는 경우가 있음**
- 이때 로드 밸런서의 **헬스 체크 방식 중 HTTP 상태 코드를 확인하는 방식**으로 로드 밸런서가 서버로 3방향 핸드셰이크를 거치고 나서 **HTTP를 요청해 정상적인 상태 코드(200 OK)를 응답하는지 여부를 체크**해 헬스 체크를 수행할 수 있음

5. **콘텐츠 확인(문자열 확인)**
- 로드 밸런서에서 서버로 컨텐츠를 요청하고 응답받은 내용을 확인하여 지정된 **콘텐츠가 정상적으로 응답했는지 여부를 확인**하는 헬스 체크 방법
- 보통 특정 웹페이지를 호출해 **사전에 지정한 문자열이 해당 웹페이지 내에 포함되어 있는지를 체크하는 기능**
- 이 헬스 체크 방식을 사용하면 로드 밸런서에서 직접 관리하는 서버의 상태 뿐만 아니라 **해당 서버의 백엔드(리얼 서버가 웹 서버인 경우, WAS 서버나 데이터베이스가 백엔드)의 상태를 해당 웹페이지로 체크**할 수 있음

한 가지 유의사항은 단순히 서버에서 응답받은 문자열만 체크하면 정상적인 요청 결괏값이 아닌 **문자열만 체크하므로 비정상적인 에러 코드에 대한 응답인 경우**라도 해당 응답 내용에 헬스 체크를 하려고 했던 **문자열이 포함되어 있으면 헬스 체크를 정상으로 판단**할 수 있다는 것

문자열을 이용한 헬스 체크를 수행할 때는

- 정상 코드 값도 중복으로 확인

- 문자열 자체를 일반적이 아닌 특정 문자열로 지정

결과가 정상일 때만 헬스 체크가 성공할 수 있도록 설정

## 12.3.2 헬스 체크 주기와 타이머

헬스 체크 주기를 볼 때는 응답 시간, 시도 횟수, 타임아웃 등 다양한 타이머를 고려해야 함

- **주기(Interval)**
    
    로드 밸런서에서 서버로 헬스 체크 패킷을 보내는 주기
    
- **응답 시간(Response)**
    
    로드 밸런서에서 서버로 헬스 체크 패킷을 보내고 응답을 기다리는 시간
    
    해당 시간까지 응답이 오지 않으면 실패로 간주
    
- **시도 횟수(Retries)**
    
    로드 밸런서에서 헬스 체크 실패 시 최대 시도 횟수
    
    최대 시도 횟수 이전에 성공 시 시도 횟수는 초기화됨
    
- **타임아웃(timeout)**
    
    로드 밸런서에서 헬스 체크 실패 시 최대 대기 시간
    
    헬스 체크 패킷을 서버로 전송한 후 이 시간 내에 성공하지 못하면 해당 서버는 다운
    
- **서비스 다운 시의 주기(Dead Interval)**
    
    서비스의 기본적인 헬스 체크 주기가 아닌, 서비스 다운 시의 헬스 체크 주기
    
    서비스가 죽은 상태에서 헬스 체크 주기를 더 늘릴 때 사용
    

<img width="885" alt="NW_pic_12-14" src="https://github.com/aivle33-dev-study/cs-study/assets/90406411/a0c1c941-1d55-416b-aac1-3ea795f2ab24">

헬스 테크 수행 타이머에 따른 주기

서비스 다운까지의 동작을 헬스 체크 주기와 시도 횟수, 응답시간으로 산정하거나 전체 타임 아웃 시간으로 산정하기도 함

헬스 체크가 실패한 첫번째 시도 시간부터 사전에 정해진 타임아웃까지 헬스 체크가 실패하면 서비스 다운으로 체크할 수도 있음

다음 두 가지 경우는 서비스 다운까지의 시간이 동일

- 주기 3초, 시도 횟수 2회, 응답시간 1초

(3초 * 3회) + 1초 = 10초

- 주기 3초, 타임아웃 10초

서비스가 다운된 후에는 기본 헬스 체크 주기마다 헬스 체크를 수행하지 않고 **더 긴 주기로 헬스 체크를 수행하는 기능**이 있는 로드 밸런서도 있음

부하를 감소시킬 수 있다는 장점이 있지만 서비스가 다시 올라오는 시간이 늦어진다는 단점도 있음

# 12.4 부하 분산 알고리즘

주요 부하 분산 알고리즘

| 라운드 로빈 (Roud Robin) | 현재 구성된 장비에 부하를 순차적으로 분산함. 총 누적 세션 수는 동일하지만 활성화된 세션 수는 달라질 수 있음 |
| --- | --- |
| 최소 접속 방식 (Least Connection) | 현재 구성된 장비 중 가장 활성화된 세션 수가 적은 장비로 부하를 분산함 |
| 가중치 기반 라운드 로빈 (Weighted Round Robin) | 라운드 로빈 방식과 동일하지만 각 장비에 가중치를 두어 가중치가 높은 장비에 부하를 더 많이 분산함. 처리 용량이 다른 서버에 부하를 분산하기 위한 분산 알고리즘 |
| 가중치 기반 최소 접속 방식 (Weighted Least Connection) | 최소 접속 방식과 동일하지만 각 장비에 가중치를 부여해 가중치가 높은 장비에 부하를 더 많이 분산함. 처리 용량이 다른 서버에 부하를 분산하기 위한 분산 알고리즘 |
| 해시 (Hash) | 해시 알고리즘을 이용한 부하 분산 |

다음은 가장 많이 사용되는 3가지 알고리즘

1. 라운드 로빈
    
    라운드 로빈 방식은 특별한 규칙 없이 현재 구성된 장비에 **순차적으로 돌아가면서 트래픽을 분산**
    
    순차적으로 모든 장비에 분산하므로 모든 장비의 총 누적 세션 수는 같아짐
    

2. 최소 접속 방식
    
    최소 접속 방식은 서버가 가진 **세션 부하를 확인**해 그것에 맞게 **부하를 분산**하는 방식
    
    로드 밸런서에서는 서비스 요청을 각 장비로 보내줄 때마다 **세션 테이블이 생성되므로** 각 장비에 연결된 **현재 세션 수를 알 수 있음**
    
    최소 접속 방식은 각 장비의 세션 수를 확인해 **현재 세션이 가장 적게 연결된 장비로 서비스 요청을 보내는 방식**
    

3. 해시
    
    해시 방식은 **서버의 부하를 고려하지 않고** 클라이언트가 **같은 서버에 지속적으로 접속**하도록 하기 위해 사용하는 부하 분산 방식
    
    서버 상태를 고려하는 것이 아니라 **해시 알고리즘을 이용해 얻은 결괏값으로 분산**할지를 결정
    
    알고리즘 계산에 사용되는 값들을 지정할 수 있는데 주로 **출발지 IP 주소, 목적지 IP 주소, 출발지 서비스 포트, 목적지 서비스 포트**를 사용
    

라운드 로빈이나 최소 접속 방식은 부하를 **비교적** 비슷한 비율로 분산

하지만 동일한 출발지에서 로드 밸런서를 거친 서비스 요청이 처음에 분산된 서버와 다음 분산된 서버가 달라질 수 있어 **각 서버에 세션을 유지해야하는 서비스는 정상적으로 서비스 되지 않음**

해시 방식은 **항상 동일한 장비로 서비스가 분산. 세션을 유지해야 하는 서비스에 적합한 분산 방식**

하지만 알고리즘 결괏값이 특정한 값으로 치우치면 부하 분산 비율이 한쪽으로 치우칠 수도 있음

해시를 사용해야 하는 이유와 최소 접속 방식의 장점을 묶어 부하 분산하는 방법도 있음

라운드 로빈 방식이나 최소 접속 방식을 사용하면서 **스티키(Stickey) 옵션을 주어 한 번 접속한 커넥션을 지속적으로 유지하는 기법**

처음 들어온 서비스 요청을 **세션 테이블에 두고** 같은 요청이 들어오면 **같은 장비로 분산해 세션을 유지**

하지만 세션 테이블의 타임아웃 옵션으로 인해 **타임아웃이 되면 분산되는 장비가 달라질 수 있다**는 것을 고려

스티키 옵션을 사용할 때는 애플리케이션 세션 유지 시간이나 일반 사용자들의 애플리케이션 행동 패턴을 충분히 감안해야함

따라서 부하 분산을 위한 알고리즘을 선택할 때는 제공되는 서비스의 특성을 잘 고려해 사용할 알고리즘을 결정해야 함

# 12.5 로드 밸런서 구성 방식

로드 밸런서의 구성 위치에 따라 2가지로 나뉨

- **원암(One-Arm) 구성 : 로드 밸런서가 중간 스위치 옆에 연결되는 구성**
- **인라인(Inline) 구성 : 서버로 가는 경로 상에 로드 밸런서가 연결되는 구성**

<img width="896" alt="NW_pic_12-20" src="https://github.com/aivle33-dev-study/cs-study/assets/90406411/c67d9a81-c4e6-48e3-829d-b1fe81ab7a39">

유의할 점: 원암이라고 단순히 로드 밸런서와 스위치 간에 연결된 인터페이스가 한 개라는 뜻은 아님

실질적으로 원암과 인라인의 구분은 **서버로 가는 트래픽이 모두 로드 밸런서를 경유하는지, 경유하지 않아도 되는지에 대한 트래픽 흐름으로 구분**인라인 구성은 부하 분산을 포함한 **모든 트래픽이 로드 밸런서를 경유**

원암 구성은 **부하 분산을 수행하는 트래픽만 로드 밸런서를 경유**

## 12.5.1 원암 구성

원암 구성은 로드 밸런서가 스위치 옆에 있는 형태

로드 밸런서가 스위치와 인터페이스 하나로 연결되어 있지만 원암 구성이 단순히 물리 인터페이스가 하나는 아님

LACP와 같은 다수의 인터페이스로 스위치와 연결된 경우에도 스위치 옆에 있는 구성이면 동일하게 원암 구성

또한 로드 밸런서와 스위치가 서로 다른 네트워크로 로드 밸런서와 구성한 경우에도 원암 구성

원암 구성에서 부하 분산을 이용하는 트래픽의 경우 부하 분산에 사용되는 서비스 IP 정보를 로드 밸런서가 가지고 있어 서버로 유입되는 트래픽은 먼저 로드 밸런서를 거침

로드 밸런서에서는 각 실제 서버로 트래픽을 분산하고 서버의 응답 트래픽은 다시 로드 밸런서를 거쳐 사용자에게 응답

로드 밸런서의 부하 분산을 이용하지 않는 트래픽은 원암 구성에서 굳이 로드 밸런서를 거치지 않음

**원암 구성은 로드 밸런서의 부하를 줄일 수 있고 인라인 방식보다 상대적으로 확장에 유리**

## 12.5.2 인라인 구성

로드 밸런서의 인라인 구성은 용어 그대로 로드 밸런서가 스위치에서 서버까지 가는 일직선상 경로에 있는 형태

인라인 구성은 트래픽이 흐르는 경로에 로드 밸런서가 있어서 서버로 향하는 트래픽이 모두 로드 밸런서를 통과

모든 트래픽이 로드 밸런서를 경유하므로 로드 밸런서의 부하가 높아짐

일반 L3 역할을 하는 스위치에 비해 로드 밸런서는 4계층 이상의 데이터를 처리하므로 처리 가능한 용량이 L3 장비보다 적으며 처리 용량이 커지면서 가격도 많이 상승

# 12.6 로드 밸런서 동작 모드

로드 밸런서 동작 방식은 3가지

- **트랜스패런트(Transparent:TP) 또는 브릿지(Bridge)**
- **라우티드(Routed)**
- **DSR(Direct Server Return)**

## 12.6.1 트랜스패런트 모드

- 로드 밸런서가 **OSI 2계층 스위치처럼 동작하는 구성**
- 로드 밸런서에서 서비스하기 위해 사용하는 VIP 주소와 실제 서버가 동일한 네트워크를 사용하는 구성
- 트랜스패런트 구성에서는 트래픽이 로드 밸런서를 지나더라도 부하 분산 서비스를 받는 트래픽인 경우에만 4계층 이상의 기능을 수행하며 부하 분산 서비스가 아닌 경우에는 기존 L2 스위치와 동일한 스위칭 기능만 수행
- L2 구조라고 부르기도 함

<img width="895" alt="NW_pic_12-27" src="https://github.com/aivle33-dev-study/cs-study/assets/90406411/6dc398e8-49b6-4cd7-82bc-720e47bf31bc">

- 트랜스패런트 모드는 **원암과 인라인 구성에서 모두 사용**할 수 있는 동작 모드
- 다만 **원암 구성**에서는 응답 트래픽 경로 부분이 문제가 될 수 있어 **Source NAT가 필요**

<img width="715" alt="NW_pic_12-28" src="https://github.com/aivle33-dev-study/cs-study/assets/90406411/230bc007-1a54-403c-864d-b0155c5a6767">

인라인 구성의 트랜스패런트 모드에서 서비스 요청 시의 패킷 흐름

1. 사용자는 서비스 IP인 로드 밸런서의 VIP 주소 10.10으로 서비스를 요청
2. 로드 밸런서로 들어온 패킷은 목적지 IP 주소를 VIP에 바인딩되어 있는 실제 서버 IP 주소로 변경하므로 목적지 IP 주소는 10.10에서 10.11로 변경
마찬가지로 목적지 MAC 주소도 실제 서버의 MAC 주소인 C가 됨
로드 밸런서와 목적지 서버가 동일한 네트워크 대역이므로 L3 장비를 지날 때처럼 출발지 MAC 주소는 변경하지 않음
3. 실제 서버로 패킷이 전달
로드 밸런서에서 서비스를 위한 VIP 주소가 실제 서버의 IP 주소로 변경해 전송하므로 목적지(Destination) NAT가 됨


<img width="727" alt="NW_pic_12-29" src="https://github.com/aivle33-dev-study/cs-study/assets/90406411/e697abdc-dcb2-45b4-b86d-f3d5febe1df2">

인라인 구성의 트랜스패런트 모드에서 서비스 응답 시의 패킷 흐름

- 서버에서 사용자에게 응답할 때는 로드 밸런서를 지나면서 요청하 때와 반대로 출발지의 IP 주소가 실제 서버의 IP에서 VIP 주소로 변경되지만 목적지 MAC 주소는 변경되지 않음. 서버에서 응답할 때, 목적지 MAC 주소가 이미 게이트웨이의 MAC 주소를 갖고 있어 변경할 필요가 없음

인라인 구성에서 로드 밸런서가 트랜스패런트 모드에서 동작할 때, 게이트웨이 외부 사용자로부터 받은 서비스 요청을 처리하는 데는 문제가 없지만 **동일 네트워크에서 서비스를 호출할 때는 서비스 응답이 로드 밸런서를 거치지 않을 수 있음**

로드 밸런서가 **원암 구성인 경우에도 서비스 응답이 로드 밸런서를 거치지 않을 수 있고** 이때 서비스에 문제가 발생할 수 있음

응답 패킷이 로드 밸런서를 다시 거쳐 역변환되어야 정상적인 부하 분산이 가능하기 때문

## 12.6.2 라우티드 모드

- **로드 밸런서가 라우팅 역할을 수행하는 모드**
- 로드 밸런서를 기준으로 사용자 방향(Client Side)과 서버 방향(Server Side)이 **서로 다른 네트워크로 분리된 구성**
- 라우티드 모드는 원암 구성과 인라인 구성에서 **모두 구성 가능**
- 라우티드 모드는 **보안 강화 목적**으로 서버쪽 넽워크를 사설로 구성해 **서버에 직접 접속하는 것을 막는 용도**로 사용하기도 함

<img width="808" alt="NW_pic_12-30" src="https://github.com/aivle33-dev-study/cs-study/assets/90406411/3239aae0-015f-4a58-b04f-dcf3545c46df">

- 윈암과 인라인 구성에서도 모두 라우티드 모드 구성이 가능

<img width="764" alt="NW_pic_12-31" src="https://github.com/aivle33-dev-study/cs-study/assets/90406411/059e3a94-4ab4-4bb2-a8cd-746d21be3660">

라우티드 모드에서 서비스 요청 시의 패킷 흐름

1. 사용자는 서비스 IP인 VIP 주소 10.10으로 서비스를 요청
2. 로드 밸런서로 들어온 패킷은 목적지 IP 주소를 VIP에 바인딩된 실제 서버 IP 주소인 20.11로 변경.라우팅을 수행하면서 로드 밸런서를 통과하므로 일반 라우팅과 동일하게 출발지와 목적지의 MAC 주소도 각각 출발지 A->D, 목적지 B->C로 변경
3. 목적지 IP와 출발지/목적지 MAC이 변경된 패킷은 라우팅 테이블을 확인해 실제 서버로 전송. 이 과정에서 로드 밸런서는 서비스를 위한 VIP에서 실제 서버의 IP 주소로 변경해 전송하므로 Destination NAT가 되었다고 함

<img width="757" alt="NW_pic_12-32" src="https://github.com/aivle33-dev-study/cs-study/assets/90406411/5ea83634-8de0-44d5-8594-318bef17cc76">

라우티드 모드에서 서비스 응답시의 패킷 흐름

1. 서버에서 사용자에게 응답하기 위해 패킷을 전송할 때는 출발지가 실제 서버의 IP주소가 되고 목적지 IP는 원래 사용자의 IP 주소가 됨. 다만 목적지 IP가 외부 네트워크이므로 목적지 MAC은 외부로 나가는 관문인 로드 밸런서의 MAC 주소가 됨

2. 로드 밸런서로 들어온 패킷은 출발지 IP 주소를 실제 서버의 IP에서 사용자가 서비스를 요청했던 VIP로 변환. 그리고 요청 트래픽과 마찬가지로 출발지와 목적지의 MAC 주소를 변경한 후 사용자에게 응답 패킷을 전송

## 12.6.3 DSR 모드

- DSR(Direct Server Return)은 사용자의 요청이 로드 밸런서를 통해 서버로 유입된 후에 **다시 로드 밸런서를 통하지 않고 서버가 사용자에게 직접 응답하는 모드**
- DSR 모드는 응답할 때, 로드 밸런서를 경유하지 않으므로 **원암으로 구성**
- DSR 모드는 **L2 DSR과 L3 DSR로 구분**
- **L2 DSR : 실제 서버의 네트워크를 로드 밸런서가 가진 경우**
- **L3 DSR : 시제 서버의 네트워크 대역을 로드 밸런서가 가지지 않은 경우**
- 로드 밸런서에서 실제 서버까지의 통신이 L2 통신인지 L3 통신인지에 따라 구분

DSR은 **로드 밸런서의 부하를 줄일 수 있는 효과**가 있는 반면에 DSR 모드의 서비스 응답이 로드 밸런서를 경유하지 않으므로 문제가 발생했을 때, **문제 확인이 어려움**

DSR은 다른 모드와 달리 서버에서도 추가 설정이 왜 필요한지 L2 DSR 모드의 트래픽 흐름을 통해 알아봄

사용자는 서비스 IP인 VIP로 서비스를 요청.

로드 밸런서로 들어온 서비스 요청 패킷은 서버에서 로드 밸런서를 거치지 않고 응답해야 하므로 응답할 때, **로드 밸런서를 통한 출발지 IP를 변경하는 Source NAT을 수행할 수 없음**

<img width="720" alt="NW_pic_12-34" src="https://github.com/aivle33-dev-study/cs-study/assets/90406411/cca6c9fc-1c5f-44d6-864b-3c7f9de8d355">

Source NAT가 수행되지 않기 때문에 사용자 입장에서는 서비스를 요청했던 IP 주소인 로드밸런서의 서비스 VIP가 아닌 **실제 서버 IP로 응답을 받음**

요청했던 IP 주소와 응답을 해주는 IP주소가 달라 비정상적인 응답으로 간주하고 패킷을 처리하지 않음

그래서 DSR 모드인 경우, 로드 밸런서는 서비스를 요청할 때 목적지 IP는 실제 서버 IP로 변경하지 않고 **VIP 그대로 유지하고 목적지 MAC 주소만 실제 서버의 MAC 주소로 변경해 서버로 전송**

서버에서는 해당 패킷을 수신할 때, 목적지 IP 주소가 서버의 주소와 맞지 않으면 폐기되므로 **루프백 인터페이스를 생성해 VIP 주소를 할당**

그리고 서비스 요청 트래픽이 들어오는 인터페이스에 설정한 IP가 아니므로 해당 인터페이스에 설정된 IP가 아닌 **루프백에 설정된 IP 주소더라도 패킷을 수신할 수 있도록 설정**

마지막으로 이 VIP는 로드 밸런서와 동일한 IP가 중복 설정된 상태이므로 ARP에 의해 중복된 IP에 대한 MAC이 갱신되지 않도록 **서버에 설정된 VIP에 대해서는 ARP 광고가 되지 않도록 함**

<img width="728" alt="NW_pic_12-35" src="https://github.com/aivle33-dev-study/cs-study/assets/90406411/6aa5f01f-af00-47cb-98e4-1e9d9ad7d5cc">

DSR 모드에서 서비스 요청 시의 패킷 흐름

1. 사용자는 서비스 IP인 VIP 주소로 서비스를 요청
2. 로드 밸런서는 목적지 IP를 VIP 주소로 두고 목적지 서버의 MAC 주소만 변경해 실제 서버로 전송
3. 실제 서버에서는 루프백 인터페이스에 VIP와 동일한 IP 주소가 설정되어 있고 목적지 IP가 이 루프백 IP와 동일한 경우에도 패킷을 수신

<img width="730" alt="NW_pic_12-36" src="https://github.com/aivle33-dev-study/cs-study/assets/90406411/32ad4c4d-5f6b-47c9-8758-ad1517403745">

DSR 모드의 응답은 로드 밸런서가 개입하지 않으므로 로드 밸런서를 사용하지 않는 일반 패킷과 유사하게 전달

다만 출발지 IP가 서버의 인터페이스 IP 주소가 아닌 **루프백 인터페이스의 IP 주소, 즉 사용자가 요청했던 VIP 주소로 설정해 패킷을 전송**

DSR 모드를 사용하려면 서버에서도 다음과 같은 추가 설정이 필요

**- 루프백 인터페이스 설정**

**- 리눅스 커널 파라미터 수정 / 네트워크 설정 변경(윈도)**

## 운영체제별 DSR 모드 구성을 위한 설정 방법

### 1. **리눅스 서버에서 루프백 인터페이스 설정**

**루프백 인터페이스 설정 : RHEL(CentOS) 계열**

```jsx
DEVICE=lo:0

IPADDR=서비스용 가상 IP(VIP)

NETMASK=255.255.255.255

ONBOOT=yes

NAME=lo0
```

**루프백 인터페이스 설정 : 데비안(우분투) 계열**

```jsx
auto lo lo:0

iface lo inet loopback

iface lo:0 inet static

address VIP

netmask 255.255.255.255
```

해당 인터페이스가 GARP(Gratuitous ARP)를 보내거나 ARP 응답을 하지않도록 설정

**커널 파라미터 추가(/etc/sysctl.conf)**

```jsx
net.ipv4.conf.lo.arp_ignore=1

net.ipv4.conf.lo.arp_announce=2

net.ipv4.conf.all.arp_ignore=1

net.ipv4.conf.all.arp_announce=2
```

**네트워크 재시작**

```jsx
systemctl network restart    또는 service network restart     # RHEL(CentOS)

service networking restart           # 데비안(우분투)
```

**윈도 서버에서 루프백 인터페이스 설정**

- 실행에서 hdwwiz를 실행
- 목록에서 직접 선택한 하드웨어 설치(고급)(M)을 선택
- 일반 하드웨어 종류에서 루프백 인터페이스도 일종의 네트워크 어댑터이므로 네트워크 어댑터를 선택
- 제조업체에서는 Microsoft를 선택하고 모델에서 Microsoft KM-TEST Loopback Adapter를 고르고 다음을 클릭해 설치

패킷을 송수신할 때, 해당 인터페이스에 설정된 IP 주소가 아니더라도 패킷을 수신하거나 송신을 허용하는 설정

설정은 네트워크 셸(netsh)을 사용

여기서는 서비스 인터페이스를 'Ethernet', 루프백 인터페이스를 'Ethernet4'라고 가정

DSR 사용을 위한 네트워크 설정 변경

```jsx
C:\>netsh interface ipv4 set interface "Ethernet" weakhostreceive=enabled

C:\>netsh interface ipv4 set interface "Ethernet4" weakhostreceive=enabled

C:\>netsh interface ipv4 set interface "Ethernet4" weakhostsend=enabled
```

변경된 설정은 다음과 같이 확인

```jsx
netsh interface ip dump
```

# 12.7 로드 밸런서 유의사항

## 12.7.1 원암 구성의 동일 네트워크 사용 시

<img width="744" alt="NW_pic_12-43" src="https://github.com/aivle33-dev-study/cs-study/assets/90406411/f79a6ba3-7d1b-46fd-b062-993f5b45f77b">


**원암 구성에서 서비스 네트워크와 서버 네트워크가 동일한 네트워크로 구성된 상황에서 발생할 수 있는 문제**

- 사용자가 서비스 IP(로드 밸런서의 VIP)로 요청하면 로드 밸런서에서는 실제 서버의 IP 주소로 Destination NAT한 후 서버로 전달
- 서버는 다시 사용자에게 응답할 때 게이트웨이 장비인 L3 스위치를 통해 응답하는데 인라인 구성에서는 로드 밸런서를 통과하지만 원암 구성에서는 로드 밸런서를 거치지 않고 사용자에게 바로 응답
- 사용자가 요청한 서비스 IP와 응답 IP는 서버의 실제 IP를 받기 때문에 서비스를 호출한 사용자 입장에서는 요청하지 않은 IP에서 응답 패킷을 받았으므로 해당 패킷은 정상적으로 처리되지 않고 폐기

- 이 문제는 로드 밸런서를 거치면서 변경된 IP가 재응답할 때, **로드 밸런서를 경유하면서 원래의 IP로 바꾸어 응답**해야 하지만 **원암 구조에서는 응답 트래픽이 로드 밸런서를 경유하지 않아서 발생**

**문제 해결하는 방법**

1. **게이트웨이를 로드 밸런서로 설정**
    
    <img width="723" alt="NW_pic_12-44" src="https://github.com/aivle33-dev-study/cs-study/assets/90406411/a5843376-655c-4702-9d8f-e61f73ebed0d">
    
    서버에서 동일 네트워크가 아닌 목적지로 가려면 게이트웨이를 통과해야 함
    
    따라서 로드 밸런서를 통해 부하 분산이 이루어지는 실제 서버에 대해서는 **게이트웨이를 로드 밸런서로 설정**하여 로컬 네트워크가 아닌 외부 사용자의 호출에 대한 응답이 **항상 로드 밸런서를 통하므로 정상적으로 응답**
    
    다만 이 경우, 원암 구조에서 가질 수 있는 로드 밸런서의 **부하 감소효과가 줄어듬**
    
    물론 부하 분산을 사용하지 않는 서버는 기존과 동일하게 게이트웨이를 L3 스위치로 설정하면 로드 밸런서를 경유하지 않으므로 여전히 로드 밸런서의 부하 감소효과를 가져옴
    

2. **Source NAT 사용**
    
    <img width="727" alt="NW_pic_12-45" src="https://github.com/aivle33-dev-study/cs-study/assets/90406411/e9c6246b-7bcc-4bfe-ab9b-aadeda404659">
    
    사용자의 서비스 요청에 대해 로드 밸런서가 실제 서버로 가기 위해 수행하는 Destination NAT뿐만 아니라 **출발지 IP 주소를 로드 밸런서가 가진 IP로 함께 변경**
    
    로드 밸런서는 응답 패킷의 출발지를 실제 서버에서 로드 밸런서에 있는 서비스 IP(VIP)로 바꾸고 목적지 IP 주소를 로드 밸런서의 IP에서 원래의 사용자 IP로 변경해 사용자에게 응답
    

다만 이 경우, 서버 애플리케이션 입장에서 보면 서비스를 호출한 IP가 하나의 동일한 IP로 보이기 때문에 **사용자 구분이 어렵다는 문제**가 있음

웹 서비스는 이런 문제를 해결하기 위해 **HTTP 헤더의 X-Forward-For(XFF)**를 사용해 실제 사용자 IP를 확인

3. **DSR 모드**
    
    <img width="730" alt="NW_pic_12-46" src="https://github.com/aivle33-dev-study/cs-study/assets/90406411/a16979c6-9576-48ef-941e-bf3a3659701f">
    
    DSR 모드는 사용자의 서비스 요청 트래픽에 대해 별도의 Destination NAT를 수행하지 않고 **실제 서버로 서비스 요청 패킷을 전송**
    
    각 서버에는 서비스 IP 정보가 루프백 인터페이스에 설정되어 있으며 서비스에 응답할 때, 루프백에 설정된 서비스 IP 주소를 출발지로 응답
    

## 12.7.2 동일 네트워크 내에서 서비스 IP(VIP) 호출

두 번째 유의사항은 동일 네트워크 내에서 서비스 IP(VIP)를 호출하는 경우

<img width="752" alt="NW_pic_12-47" src="https://github.com/aivle33-dev-study/cs-study/assets/90406411/d00512e0-ca1e-4f98-a1c6-fcd24f04f843">

서버 #1은 로드 밸런서의 서비스 IP를 통해 부하 분산이 이루어지고 있는 서버

1. 서버 #2에서 서버 #1의 서비스 IP 호출을 위해 로드 밸런서로 서비스를 요청

2. 로드 밸런서에서는 목적지 IP인 서비스 IP 주소를 서버 #1의 IP 주소로 변환해 서버 #1로 전달

3. 서비스 요청을 받은 서버 #1은 서비스를 호출한 출발지 IP를 확인해 응답하는데 이때 서비스를 호출한 출발지가 자신과 동일한 네트워크임을 확인. 동일한 네트워크이므로 목적지에 대해 로드 밸런서를 거치지 않고 바로 응답

4. 서버 #2에서는 서비스를 요청한 IP 주소가 아닌 다른 IP 주소로 응답이 오므로 해당 패킷은 폐기 되면서 정상적인 서비스가 이루어지지 않음

이런 문제는 원암 구성이든 인라인 구성이든 모두 발생

이 문제의 해결 방법은 앞의 해결 방법과 거의 동일

**1. 서비스 요청이 로드 밸런서를 거칠 때, 출발지 IP 주소를 로드 밸런서의 IP로 변경하는 Source NAT 방법**

**2. DSR 모드를 사용해 실제 서버에서 로드 밸런서를 거치지 않고 직접 응답**

**3. 부하 분산 서비스를 받는 서버를 로드 밸런서에 직접 연결해 어떤 서비스 요청에 대한 응답이든 물리적으로 로드 밸런서를 거치게 하는 것**

# 12.8 HAProxy를 사용한 로드 밸런서 설정

- HAProxy는 기존 하드웨어 로드 밸런서의 역할을 **일반 서버에서 직접 수행**하게 해주는 **오픈 소스 기반의 소프트웨어 로드 밸런서**
- 하드웨어 로드 밸런서에서 제공되는 기능을 소프트웨어로 제공하므로 일종의 NFV(Network Function Virtualization)라고 볼 수 있음
- 소프트웨어 형태이므로 가상화나 클라우드 환경에서 로드 밸런서로 사용하기에 매우 적합한 솔루션
- 또한, 쿠버네티스의 인그레스 컨트롤러(Ingress Controller) 역할도 할 수 있음

## 12.8.1 HAProxy 설치

일반 패키지 설치와 동일하게 yum을 사용해 설치

```jsx
# yum install haproxy
```

yum으로 설치 가능한 HAProxy 버전 확인

```jsx
# yum info haproxy
```

최신 버전의 HAProxy를 사용하면 yum을 이요하지 않고 최신 소스를 직접 받아 컴파일해야 함

컴파일을 하기위한 패키지 설치

```jsx
# yum -y install gcc
```

최신 버전의 HAProxy 설치

```jsx
# wget http://www.haproxy.org/download/2.1/src/haproxy-2.1.4.tar.gz
```

소스 코드의 압축 해제

```jsx
# tar xvzf haproxy-2.1.4.tar.gz
```

HAProxy를 컴파일

```jsx
# cd haproxy-2.1.4/

# make TARGET=linux-glibc
```

make install 명령어로 설치

```jsx
# make install
```

설치된 버전 확인

```jsx
# haproxy -v
```

/usr/sbin 디렉터리에 haproxy의 심볼릭 링크를 설정

```jsx
# ln -s /usr/local/sbin/haproxy /usr/sbin/haproxy
```

haproxy를 서비스로 등록하기 위해 haproxy 설치 디렉터리에 있는 example 디렉터리의 haproxy.init 파일을 inid.d 디렉터리에 복사하고 권한(Permission)을 변경한 후 데몬을 재시작

```jsx
# cd ~/haproxy-2.1.4/examples/haproxy.init /etc/init.d/haproxy

# chmod 755 /etc/init.d/haproxy

# systemctl daemon-reload
```

HAProxy 환경 설정 및 통계 값을 위한 디렉토리 및 파일 생성

```jsx
# mkdir -p /etc/haproxy

# mkdir -p /var/lib/haproxy

# touch /var/lib/haproxy/stat
```

기본 설치 작업 완료

## 12.8.3 HAProxy 설정

HAProxy는 haproxy.cfg 파일에 기본 속서오가 부하 분산 설정을 하고 HAProxy 서비스를 실행하면 이 설정 값을 불러와 구동

기본 설정 파일 경로

**- /etc/haproxy/haproxy.cfg**

HAProxy 설정 파일의 섹션
